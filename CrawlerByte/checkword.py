import json


def check_word_in_tokenized_data(word_to_check, json_file_path):
    # Load tokenized data from the JSON file generated by main.py
    with open(json_file_path, "r", encoding="utf-8") as json_file:
        tokenized_data = json.load(json_file)

    # Ensure the loaded data is a dictionary
    if not isinstance(tokenized_data, dict):
        raise ValueError(
            "The JSON file does not contain valid tokenized data.")

    # Extract words from the tokenized data
    words = tokenized_data.get("Words", [])

    # Convert the word_to_check and words to lowercase for case-insensitive comparison
    word_to_check_lower = word_to_check.lower()
    words_lower = [word.lower() for word in words]

    # Check if the word_to_check is in the tokenized set of words
    if word_to_check_lower in words_lower:
        print(
            f"The word '{word_to_check}' is present in the tokenized set of words.")
    else:
        print(
            f"The word '{word_to_check}' is NOT present in the tokenized set of words.")
    